{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importing packages:\n",
    "from skimage.segmentation import slic,mark_boundaries\n",
    "from skimage.util import img_as_float\n",
    "from skimage import io\n",
    "from skimage.measure import regionprops\n",
    "from skimage.color import rgb2hsv\n",
    "from sklearn.model_selection import StratifiedKFold,StratifiedShuffleSplit,KFold,ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "import cv2 as cv \n",
    "import os,glob, shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "imagename= \"horse/rgb/horse001.jpg\"\n",
    "labeling= \"horse/figure_ground/horse001.jpg\"\n",
    "n_segments =100\n",
    "sigma = 5 \n",
    "def preprocessing(imagename, labeling,n_segments,sigma):\n",
    "    # load the image and convert it to a floating point data type\n",
    "    image= img_as_float(io.imread(imagename))\n",
    "    #load the image with the labeling\n",
    "    imlabels= cv.imread(labeling)\n",
    "    imlabels= cv.resize(imlabels,(image.shape[1],image.shape[0]))\n",
    "    # Perform slic super pixels segmentations\n",
    "    segments = slic(image, n_segments = n_segments, sigma = sigma)\n",
    "    regionpro=regionprops(segments,imlabels[:,:,1])\n",
    "    prob=np.array([p.mean_intensity/255 for p in regionpro])\n",
    "    prob = np.insert(prob, 0, [0])\n",
    "    labels=[0 if i<0.5 else 1 for i in prob]\n",
    "    return image,segments,labels\n",
    "\n",
    "def make3d(descriptor):\n",
    "    if (descriptor.ndim==2):\n",
    "        dim=1\n",
    "    else:\n",
    "        dim=descriptor.shape[2]\n",
    "    return np.reshape(descriptor, (descriptor.shape[0],descriptor.shape[1],dim))\n",
    "\n",
    "\n",
    "def compute_polling(plane, segments):\n",
    "    numsegments = int(np.max(segments)+1)\n",
    "    fv = np.zeros((5,numsegments))\n",
    "    fv[3,:] = np.max(plane)\n",
    "    fv[4,:] = np.min(plane)\n",
    "    for y in range(plane.shape[0]):\n",
    "        for x in range(plane.shape[1]):\n",
    "            L = int(segments[y,x])\n",
    "            V = plane[y,x]\n",
    "            fv[0,L] = fv[0,L]+1\n",
    "            fv[1,L] = fv[1,L]+V\n",
    "            fv[2,L] = fv[2,L]+V**2\n",
    "            fv[3,L] = min(fv[3,L],V)\n",
    "            fv[4,L] = max(fv[4,L],V)\n",
    "    #for L in range(numsegments):\n",
    "    fv[1,:] = fv[1,:]/fv[0,:]\n",
    "    fv[2,:] = fv[2,:]-fv[1,:]**2\n",
    "    fv[np.isnan(fv)] = 0\n",
    "    return fv\n",
    "\n",
    "\n",
    "def build_features(image, segments):\n",
    "    hsv = rgb2hsv(image)\n",
    "    h=hsv[:,:,0]/180\n",
    "    s=hsv[:,:,1]/255\n",
    "    v=hsv[:,:,2]/255\n",
    "    ga=cv.GaussianBlur(image,(61,61),8.0)\n",
    "    laga=cv.Laplacian(ga,cv.CV_64F)\n",
    "    sobel= cv.Sobel(image,cv.CV_64F,1,0,ksize=3)\n",
    "    descriptors=[image,h,s,v,ga,laga,sobel]  \n",
    "    dataset=np.zeros((5,segments.max()+1))\n",
    "    count=0\n",
    "    for descriptor in descriptors:\n",
    "        descriptor = make3d(descriptor)\n",
    "        for channel in range(0,descriptor.shape[2]):\n",
    "            plane = descriptor[:,:,channel]\n",
    "            fv=compute_polling(plane,segments)\n",
    "            dataset=np.r_[dataset,fv]\n",
    "    return dataset\n",
    "\n",
    "def build_dataset(imagespath,labelingpath):\n",
    "    imlist = glob.glob(os.path.join(imagespath,'*.jpg'))\n",
    "    data  = np.ndarray((0,80))\n",
    "    labeling = []\n",
    "    for im in imlist[0:20]:\n",
    "        image,segments,labels = preprocessing(im,labelingpath+im[10:],n_segments,sigma)\n",
    "        dataset = np.transpose(build_features(image,segments))\n",
    "        data = np.r_[data,dataset]\n",
    "        labeling += labels\n",
    "    return data,labeling\n",
    "\n",
    "def classify(dataset,labels):\n",
    "    k = 5\n",
    "    n_splits=5\n",
    "    skf = StratifiedKFold(n_splits=n_splits)\n",
    "    skf_ss = StratifiedShuffleSplit(n_splits=n_splits)\n",
    "    C=1\n",
    "    classifiers={\"knn\":KNeighborsClassifier(n_neighbors=k),\"svml\":svm.SVC(kernel='linear', C=C)\n",
    "                 ,\"svmr\":svm.SVC(kernel='rbf', C=C),\n",
    "                   \"boost\":GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                                             max_depth=2, random_state=0)}\n",
    "    scores={}\n",
    "    for k in classifiers.keys():\n",
    "        sc= cross_val_score(classifiers[k],dataset,labels,cv=skf)\n",
    "        sc2= cross_val_score(classifiers[k],dataset,labels,cv=skf_ss)\n",
    "        scores[k]=[sc.mean(),sc.std()]\n",
    "        scores[\"shuffling \"+ k]=[sc2.mean(),sc2.std()]\n",
    "    return scores\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Image Example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boost': [0.92573099415204685, 0.053456818161527211],\n",
       " 'knn': [0.84093567251461998, 0.03147046560718994],\n",
       " 'shuffling boost': [0.98000000000000009, 0.039999999999999994],\n",
       " 'shuffling knn': [0.90000000000000002, 0.0],\n",
       " 'shuffling svml': [0.91999999999999993, 0.074833147735478819],\n",
       " 'shuffling svmr': [0.90000000000000002, 0.0],\n",
       " 'svml': [0.87192982456140344, 0.10373154736206651],\n",
       " 'svmr': [0.85146198830409359, 0.018713450292397661]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagename= \"horse/rgb/horse001.jpg\"\n",
    "labeling= \"horse/figure_ground/horse001.jpg\"\n",
    "n_segments =100\n",
    "sigma = 5 \n",
    "image,segments,labels=preprocessing(imagename,labeling,n_segments,sigma)\n",
    "dataset=build_features(image,segments)\n",
    "data=np.transpose(dataset)\n",
    "classify(data,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21 Images Example :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imagespath = \"horse/rgb/\"\n",
    "labelingpath = \"horse/figure_ground/\"\n",
    "data,labels = build_dataset(imagespath,labelingpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classify(data,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiler : Timer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "from time import time\n",
    "def timing(f):\n",
    "    @wraps(f)\n",
    "    def wrap(*args, **kw):\n",
    "        ts = time()\n",
    "        result = f(*args, **kw)\n",
    "        te = time()\n",
    "        print ('func:%r args:[%r, %r] took: %2.4f sec' % \\\n",
    "          (f.__name__, args, kw, te-ts))\n",
    "        return result\n",
    "    return wrap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except ImportError:\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# libact classes\n",
    "from libact.base.dataset import Dataset, import_libsvm_sparse\n",
    "from libact.models import *\n",
    "from libact.query_strategies import *\n",
    "from libact.labelers import IdealLabeler\n",
    "\n",
    "def run(trn_ds, tst_ds, lbr, model, qs, quota):\n",
    "    E_in, E_out = [], []\n",
    "\n",
    "    for _ in range(quota):\n",
    "        # Standard usage of libact objects\n",
    "        ask_id = qs.make_query()\n",
    "        X, _ = zip(*trn_ds.data)\n",
    "        lb = lbr.label(X[ask_id])\n",
    "        trn_ds.update(ask_id, lb)\n",
    "\n",
    "        model.train(trn_ds)\n",
    "        E_in = np.append(E_in, 1 - model.score(trn_ds))\n",
    "        E_out = np.append(E_out, 1 - model.score(tst_ds))\n",
    "\n",
    "    return E_in, E_out\n",
    "\n",
    "\n",
    "def split_train_test(X,y, test_size, n_labeled):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "    trn_ds = Dataset(X_train, np.concatenate([y_train[:n_labeled], [None] * (len(y_train) - n_labeled)]))\n",
    "    tst_ds = Dataset(X_test, y_test)\n",
    "    fully_labeled_trn_ds = Dataset(X_train, y_train)\n",
    "\n",
    "    return trn_ds, tst_ds, y_train, fully_labeled_trn_ds\n",
    "def active_learning(data,labels,test_size,n_labeled):\n",
    "    # Load dataset\n",
    "    trn_ds, tst_ds, y_train, fully_labeled_trn_ds = split_train_test(data,labels, test_size, n_labeled)\n",
    "    trn_ds2 = copy.deepcopy(trn_ds)\n",
    "    lbr = IdealLabeler(fully_labeled_trn_ds)\n",
    "\n",
    "    quota = len(y_train) - n_labeled    # number of samples to query\n",
    "\n",
    "    # Comparing UncertaintySampling strategy with RandomSampling.\n",
    "    # model is the base learner, e.g. LogisticRegression, SVM ... etc.\n",
    "    qs = UncertaintySampling(trn_ds, method='lc', model=LogisticRegression())\n",
    "    model = LogisticRegression()\n",
    "    E_in_1, E_out_1 = run(trn_ds, tst_ds, lbr, model, qs, quota)\n",
    "\n",
    "    qs2 = RandomSampling(trn_ds2)\n",
    "    model = LogisticRegression()\n",
    "    E_in_2, E_out_2 = run(trn_ds2, tst_ds, lbr, model, qs2, quota)\n",
    "\n",
    "    # Plot the learning curve of UncertaintySampling to RandomSampling\n",
    "    # The x-axis is the number of queries, and the y-axis is the corresponding\n",
    "    # error rate.\n",
    "    query_num = np.arange(1, quota + 1)\n",
    "    plt.plot(query_num, E_in_1, 'b', label='qs Ein')\n",
    "    plt.plot(query_num, E_in_2, 'r', label='random Ein')\n",
    "    plt.plot(query_num, E_out_1, 'g', label='qs Eout')\n",
    "    plt.plot(query_num, E_out_2, 'k', label='random Eout')\n",
    "    plt.xlabel('Number of Queries')\n",
    "    plt.ylabel('Error')\n",
    "    plt.title('Experiment Result')\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05),\n",
    "           fancybox=True, shadow=True, ncol=5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-b7b8814ed493>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-b7b8814ed493>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def active_learning(data,labels,test_size,n_labeled)\u001b[0m\n\u001b[0m                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def active_learning(data,labels,test_size,n_labeled)\n",
    "\n",
    "    # Load dataset\n",
    "    trn_ds, tst_ds, y_train, fully_labeled_trn_ds = split_train_test(data,labels, test_size, n_labeled)\n",
    "    trn_ds2 = copy.deepcopy(trn_ds)\n",
    "    lbr = IdealLabeler(fully_labeled_trn_ds)\n",
    "\n",
    "    quota = len(y_train) - n_labeled    # number of samples to query\n",
    "\n",
    "    # Comparing UncertaintySampling strategy with RandomSampling.\n",
    "    # model is the base learner, e.g. LogisticRegression, SVM ... etc.\n",
    "    qs = UncertaintySampling(trn_ds, method='lc', model=LogisticRegression())\n",
    "    model = LogisticRegression()\n",
    "    E_in_1, E_out_1 = run(trn_ds, tst_ds, lbr, model, qs, quota)\n",
    "\n",
    "    qs2 = RandomSampling(trn_ds2)\n",
    "    model = LogisticRegression()\n",
    "    E_in_2, E_out_2 = run(trn_ds2, tst_ds, lbr, model, qs2, quota)\n",
    "\n",
    "    # Plot the learning curve of UncertaintySampling to RandomSampling\n",
    "    # The x-axis is the number of queries, and the y-axis is the corresponding\n",
    "    # error rate.\n",
    "    query_num = np.arange(1, quota + 1)\n",
    "    plt.plot(query_num, E_in_1, 'b', label='qs Ein')\n",
    "    plt.plot(query_num, E_in_2, 'r', label='random Ein')\n",
    "    plt.plot(query_num, E_out_1, 'g', label='qs Eout')\n",
    "    plt.plot(query_num, E_out_2, 'k', label='random Eout')\n",
    "    plt.xlabel('Number of Queries')\n",
    "    plt.ylabel('Error')\n",
    "    plt.title('Experiment Result')\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05),\n",
    "           fancybox=True, shadow=True, ncol=5)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole Dataset -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=np.loadtxt(\"data.txt\")\n",
    "labels=np.loadtxt(\"labels.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_size = 0.25    # the percentage of samples in the dataset that will be\n",
    "                    # randomly selected and assigned to the test set\n",
    "n_labeled = 10 # number of samples that are initially labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "active_learning(data,labels,test_size,n_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
